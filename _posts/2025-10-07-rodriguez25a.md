---
title: Cross-Sensor Touch Generation
section: Oral
openreview: oGcC8nMOit
abstract: 'Todayâ€™s visuo-tactile sensors come in many shapes and sizes, making it
  challenging to develop general-purpose tactile representations. This is because
  most models are tied to a specific sensor design. To address this challenge, we
  propose two approaches to cross-sensor image generation. The first is an end-to-end
  method that leverages paired data (Touch2Touch). The second method builds an intermediate
  depth representation and does not require paired data (T2D2: Touch-to-Depth-to-Touch).
  Both methods enable the use of sensor-specific models across multiple sensors via
  the cross-sensor touch generation process. Together, these models offer flexible
  solutions for sensor translation, depending on data availability and application
  needs. We demonstrate their effectiveness on downstream tasks such as cup stacking
  and tool insertion, where models originally designed for one sensor are successfully
  transferred to another using in-hand pose estimation.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rodriguez25a
month: 0
tex_title: Cross-Sensor Touch Generation
firstpage: 152
lastpage: 167
page: 152-167
order: 152
cycles: false
bibtex_author: Rodriguez, Samanta and Dou, Yiming and Oller, Miquel and Owens, Andrew
  and Fazeli, Nima
author:
- given: Samanta
  family: Rodriguez
- given: Yiming
  family: Dou
- given: Miquel
  family: Oller
- given: Andrew
  family: Owens
- given: Nima
  family: Fazeli
date: 2025-10-07
address:
container-title: Proceedings of The 9th Conference on Robot Learning
volume: '305'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 10
  - 7
pdf: https://raw.githubusercontent.com/mlresearch/v305/main/assets/rodriguez25a/rodriguez25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
