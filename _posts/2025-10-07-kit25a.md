---
title: 'EndoVLA: Dual-Phase Vision-Language-Action for Precise Autonomous Tracking
  in Endoscopy'
section: Poster
openreview: 7XyO9Y1hI1
abstract: 'In endoscopic procedures, autonomous tracking of abnormal regions and following
  of circumferential cutting markers can significantly reduce the cognitive burden
  on endoscopists. However, conventional model-based pipelines are fragile—each component
  (e.g., detection, motion planning) requires manual tuning and struggles to incorporate
  high-level endoscopic intent, resulting in poor generalization across variable scenes.
  Vision–Language–Action (VLA) models, which integrate visual perception, language
  grounding, and motion planning within an end-to-end framework, offer a promising
  alternative to semantically adapt to surgeon prompts, without the need for manual
  recalibration. Despite their potential, applying VLA models to robotic endoscopy
  presents unique challenges due to the inherently complex and dynamic anatomical
  environments of the gastrointestinal (GI) tract. To this end, we introduce EndoVLA,
  designed specifically for continuum robots in GI interventions. Provided endoscopic
  images and surgeon-issued tracking prompts, EndoVLA performs three core tasks: (1)
  polyp tracking, (2) delineation and following of abnormal mucosal regions, and (3)
  adherence to predefined circular markers during circumferential cutting. To address
  the unique challenges posed by data scarcity and domain shifts, we propose a dual-phase
  strategy, with supervised fine-tuning on our EndoVLA-Motion dataset and reinforcement
  fine-tuning using task-aware rewards. Our approach significantly enhances the tracking
  performance in endoscopy, and zero-shot generalization of tracking in general scenes
  and more challenging sequential tasks.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kit25a
month: 0
tex_title: 'EndoVLA: Dual-Phase Vision-Language-Action for Precise Autonomous Tracking
  in Endoscopy'
firstpage: 4958
lastpage: 4974
page: 4958-4974
order: 4958
cycles: false
bibtex_author: KIT, NG CHI and Bai, Long and Wang, Guankun and Wang, Yupeng and Gao,
  Huxin and yuan, Kun and Jin, Chenhan and Zeng, Tieyong and Ren, Hongliang
author:
- given: NG CHI
  family: KIT
- given: Long
  family: Bai
- given: Guankun
  family: Wang
- given: Yupeng
  family: Wang
- given: Huxin
  family: Gao
- given: Kun
  family: yuan
- given: Chenhan
  family: Jin
- given: Tieyong
  family: Zeng
- given: Hongliang
  family: Ren
date: 2025-10-07
address:
container-title: Proceedings of The 8th Conference on Robot Learning
volume: '305'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 10
  - 7
pdf: https://raw.githubusercontent.com/mlresearch/v305/main/assets/kit25a/kit25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
