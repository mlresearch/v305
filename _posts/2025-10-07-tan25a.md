---
title: 'Search-TTA: A Multi-Modal Test-Time Adaptation Framework for Visual Search
  in the Wild'
section: Poster
openreview: iVbCWUDyBF
abstract: To perform autonomous visual search for environmental monitoring, a robot
  may leverage satellite imagery as a prior map. This can help inform coarse, high
  level search and exploration strategies, even when such images lack sufficient resolution
  to allow fine-grained, explicit visual recognition of targets. However, there are
  some challenges to overcome with using satellite images to direct visual search.
  For one, targets that are unseen in satellite images are underrepresented (compared
  to real life) in most existing datasets, and thus vision models trained on these
  datasets fail to reason effectively based on indirect visual cues. Furthermore,
  approaches which leverage large Vision Language Models (VLMs) for generalization
  may yield inaccurate outputs due to hallucination, leading to inefficient search.
  To address these challenges, we introduce Search-TTA, a multimodal test-time adaptation
  framework that can accept text and/or image input. First, we pretrain a remote sensing
  image encoder to align with CLIP’s visual encoder to output probability distributions
  of target presence used for visual search. Second, our framework dynamically refines
  CLIP’s predictions during search using a test-time adaptation mechanism. Through
  a feedback loop inspired by Spatial Poisson Point Processes, gradient updates (weighted
  by uncertainty) are used to correct (potentially inaccurate) predictions and improve
  search performance. To validate Search-TTA’s performance, we curate a visual search
  dataset based on internet-scale ecological data. We find that Search-TTA improves
  planner performance by up to 9.7%, particularly in cases with poor initial CLIP
  predictions. It also achieves comparable performance to state-of-the-art VLMs. Finally,
  we deploy Search-TTA on a real UAV via hardware-in-the-loop testing, by simulating
  its operation within a large-scale simulation that provides onboard sensing.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tan25a
month: 0
tex_title: 'Search-TTA: A Multi-Modal Test-Time Adaptation Framework for Visual Search
  in the Wild'
firstpage: 2093
lastpage: 2120
page: 2093-2120
order: 2093
cycles: false
bibtex_author: Tan, Derek Ming Siang and Shailesh, Shailes and Liu, Boyang and Raj,
  Alok and Ang, Qi Xuan and Dai, Weiheng and Duhan, Tanishq and Chiun, Jimmy and Cao,
  Yuhong and Shkurti, Florian and Sartoretti, Guillaume Adrien
author:
- given: Derek Ming Siang
  family: Tan
- given: Shailes
  family: Shailesh
- given: Boyang
  family: Liu
- given: Alok
  family: Raj
- given: Qi Xuan
  family: Ang
- given: Weiheng
  family: Dai
- given: Tanishq
  family: Duhan
- given: Jimmy
  family: Chiun
- given: Yuhong
  family: Cao
- given: Florian
  family: Shkurti
- given: Guillaume Adrien
  family: Sartoretti
date: 2025-10-07
address:
container-title: Proceedings of The 9th Conference on Robot Learning
volume: '305'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 10
  - 7
pdf: https://raw.githubusercontent.com/mlresearch/v305/main/assets/tan25a/tan25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
