---
title: '3DS-VLA: A 3D Spatial-Aware Vision Language Action Model for Robust Multi-Task
  Manipulation'
section: Poster
openreview: dT45OMevL5
abstract: Recently, 2D vision-language-action (VLA) models have made significant strides
  in multi-task manipulation. However, these models struggle to reason about 3D spatial
  relationships from 2D image inputs. Although an increasing number of 3D approaches
  explicitly integrate 3D information, they encounter challenges such as limited availability
  of large-scale 3D datasets and loss of spatial information during input processing.
  Meanwhile, existing policies typically focus on the perception-to-action learning
  paradigm, lacking an explicit understanding of the spatial and temporal relationships
  between the robot and its environment. To address this, we propose 3DS-VLA, which
  enhances pretrained 2D vision-language models (VLMs) with comprehensive 3D awareness,
  enabling the prediction of robust end-effector poses. Specifically, we enable a
  2D vision encoder to encode both 2D images and 3D spatial observation by introducing
  a 2D-to-3D positional alignment mechanism. This allows 3DS-VLA to leverage the large-scale
  pre-trained knowledge of the VLM for effective reasoning in complex 3D robotic environments.
  Furthermore, to better understand the spatiotemporal relationship between 3D observations
  and robot behavior, we guide the model to learn the introduced sequential 3D spatial
  constraints, which define affordance-relevant 3D keypoints on objects, ensuring
  robust interactions. Experiments in simulated and real-world demonstrate that 3DS-VLA
  outperforms previous state-of-the-art policies and showcase its generalizable capabilities
  across multi-task, multi-embodiment, and diverse environmental settings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li25g
month: 0
tex_title: '3DS-VLA: A 3D Spatial-Aware Vision Language Action Model for Robust Multi-Task
  Manipulation'
firstpage: 2344
lastpage: 2359
page: 2344-2359
order: 2344
cycles: false
bibtex_author: Li, Xiaoqi and Heng, Liang and Liu, Jiaming and Shen, Yan and Gu, Chenyang
  and Liu, Zhuoyang and Chen, Hao and Han, Nuowei and Zhang, Renrui and Tang, Hao
  and Zhang, Shanghang and Dong, Hao
author:
- given: Xiaoqi
  family: Li
- given: Liang
  family: Heng
- given: Jiaming
  family: Liu
- given: Yan
  family: Shen
- given: Chenyang
  family: Gu
- given: Zhuoyang
  family: Liu
- given: Hao
  family: Chen
- given: Nuowei
  family: Han
- given: Renrui
  family: Zhang
- given: Hao
  family: Tang
- given: Shanghang
  family: Zhang
- given: Hao
  family: Dong
date: 2025-10-07
address:
container-title: Proceedings of The 9th Conference on Robot Learning
volume: '305'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 10
  - 7
pdf: https://raw.githubusercontent.com/mlresearch/v305/main/assets/li25g/li25g.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
