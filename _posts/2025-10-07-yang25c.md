---
title: Extracting Visual Plans from Unlabeled Videos via Symbolic Guidance
section: Poster
openreview: HMcBBIg1Th
abstract: Visual planning, by offering a sequence of intermediate visual subgoals
  to a goal-conditioned low-level policy, achieves promising performance on long-horizon
  manipulation tasks. To obtain the subgoals, existing methods typically resort to
  video generation models but suffer from model hallucination and computational cost.
  We present Vis2Plan, an efficient, explainable and white-box visual planning framework
  powered by symbolic guidance. From raw, unlabeled play data, Vis2Plan harnesses
  vision foundation models to automatically extract a compact set of task symbols,
  which allows building a high-level symbolic transition graph for multi-goal, multi-stage
  planning. At test time, given a desired task goal, our planner conducts planning
  at the symbolic level and assembles a sequence of physically consistent intermediate
  sub-goal images grounded by the underlying symbolic representation. Our Vis2Plan
  outperforms strong diffusion video generation-based visual planners by delivering
  53% higher aggregate success rate while generating visual plans 35$\times$ faster.
  The results indicate that Vis2Plan is able to generate physically consistent image
  goals while offering fully inspectable reasoning steps.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang25c
month: 0
tex_title: Extracting Visual Plans from Unlabeled Videos via Symbolic Guidance
firstpage: 3995
lastpage: 4018
page: 3995-4018
order: 3995
cycles: false
bibtex_author: Yang, Wenyan and Tikna, Ahmet and Zhao, Yi and Zhang, Yuying and Palopoli,
  Luigi and Roveri, Marco and Pajarinen, Joni
author:
- given: Wenyan
  family: Yang
- given: Ahmet
  family: Tikna
- given: Yi
  family: Zhao
- given: Yuying
  family: Zhang
- given: Luigi
  family: Palopoli
- given: Marco
  family: Roveri
- given: Joni
  family: Pajarinen
date: 2025-10-07
address:
container-title: Proceedings of The 8th Conference on Robot Learning
volume: '305'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 10
  - 7
pdf: https://raw.githubusercontent.com/mlresearch/v305/main/assets/yang25c/yang25c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
